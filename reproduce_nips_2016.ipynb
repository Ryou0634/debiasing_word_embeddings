{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Specify the data path in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "WORD2VEC_DATA_PATH = \"SOMEWHERE_ON_YOUR_COMPUTER/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "DEFINITIONAL_PAIRS_PATH = \"data/definitional_pairs.json\"\n",
    "PROFESSIONS_DATA_PATH = \"data/professions.json\"\n",
    "EQUALIZE_PAIRS_PATH = \"data/equalize_pairs.json\"\n",
    "GENDER_SPECIFIC_DATA_PATH = \"data/gender_specific_full.json\"\n",
    "\n",
    "def read_json(data_path: str):\n",
    "    return json.load(open(data_path, 'r'))\n",
    "\n",
    "definitional_pairs = read_json(DEFINITIONAL_PAIRS_PATH)\n",
    "professions = read_json(PROFESSIONS_DATA_PATH)\n",
    "equalize_pairs = read_json(EQUALIZE_PAIRS_PATH)\n",
    "gender_specific = set(read_json(GENDER_SPECIFIC_DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preliminaries\n",
    "\n",
    "### Embedding\n",
    "Unless otherwise stated, the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec embedding. In particular, we downloaded the pre-trained embedding on the Google News corpus (https://code.google.com/archive/p/word2vec/), and normalized each word to unit length as is common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(WORD2VEC_DATA_PATH, limit=100000, binary=True)\n",
    "model.init_sims(replace=True) # this normalizes the vectors to be the unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Gender stereotypes in word embeddings\n",
    "### Occupational stereotypes.\n",
    "Figure 1 lists the occupations that are closest to _she_ and to _he_ in the w2vNEWS embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def get_most_similar_words_with(target_word: str, query_words: List[str], model: KeyedVectors, top_k: int = 12):\n",
    "    similarities = Counter()\n",
    "    for word in query_words:\n",
    "        similarities[word] = model.similarity(word, target_word)\n",
    "    return similarities.most_common(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = [word for word, _, _ in professions if word not in gender_specific]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: The most extreme occupations as projected on to the _she−he_ gender direction on g2vNEWS.  \n",
    "Occupations such as _businesswoman,_ where gender is suggested by the orthography, were excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homemaker', 0.398446),\n",
       " ('receptionist', 0.38378775),\n",
       " ('hairdresser', 0.37579125),\n",
       " ('nurse', 0.3694259),\n",
       " ('registered_nurse', 0.35921058),\n",
       " ('confesses', 0.34259766),\n",
       " ('paralegal', 0.32918584),\n",
       " ('housekeeper', 0.32787254),\n",
       " ('clerk', 0.31268093),\n",
       " ('nanny', 0.2962942),\n",
       " ('dancer', 0.29316145),\n",
       " ('bookkeeper', 0.29155013)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_similar_words_with(\"she\", occupations, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soft_spoken', 0.35008138),\n",
       " ('protege', 0.2988804),\n",
       " ('politician', 0.296749),\n",
       " ('sportsman', 0.2909003),\n",
       " ('confesses', 0.28672272),\n",
       " ('actor', 0.2804842),\n",
       " ('doctor', 0.2743739),\n",
       " ('ballplayer', 0.27259606),\n",
       " ('carpenter', 0.27101207),\n",
       " ('policeman', 0.27044824),\n",
       " ('cabbie', 0.26954412),\n",
       " ('priest', 0.2612976)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_similar_words_with(\"he\", occupations, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies exhibiting stereotypes.\n",
    "Figure 2: Analogy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('businessman', 0.5175440311431885),\n",
       " ('schoolteacher', 0.4978965222835541),\n",
       " ('shopkeeper', 0.47978055477142334),\n",
       " ('laborer', 0.4709293842315674),\n",
       " ('man', 0.4704824686050415),\n",
       " ('taxi_driver', 0.46757081151008606),\n",
       " ('carpenter', 0.45320457220077515),\n",
       " ('farmer', 0.4418044984340668),\n",
       " ('hero', 0.4327959418296814),\n",
       " ('lad', 0.42618119716644287)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['he', 'housewife'], negative=['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseball', 0.6792116165161133),\n",
       " ('football', 0.5886937379837036),\n",
       " ('Softball', 0.5582471489906311),\n",
       " ('basketball', 0.5443937182426453),\n",
       " ('soccer', 0.539060115814209),\n",
       " ('Baseball', 0.5169258117675781),\n",
       " ('volleyball', 0.5008956789970398),\n",
       " ('batting_cages', 0.49255841970443726),\n",
       " ('Cal_Ripken', 0.4907674491405487),\n",
       " ('junior_varsity', 0.48719659447669983)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['he', 'softball'], negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indirect gender bias\n",
    "To test this indirect gender bias, we take pairs of words that are gender-neutral, for example _softball_ and _football._ We project all the occupation words onto the _softball_ − _football_ direction and looked at the extremes words, which are listed in Figure 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_between_two_points(vec: np.ndarray, a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    The output is the cooordinate between the two points a and b.\n",
    "    The output 0 means the center of a and b, 1 means b, -1 means a.\n",
    "    \"\"\"\n",
    "    new_origin = (a + b) / 2\n",
    "    new_axis = b - new_origin\n",
    "    new_axis_unit_length = np.linalg.norm(new_axis)\n",
    "    new_axis /= new_axis_unit_length\n",
    "    vec_from_new_origin = vec - new_origin\n",
    "    return vec_from_new_origin.dot(new_axis) / new_axis_unit_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotted_occupations = Counter()\n",
    "for word in occupations:\n",
    "    plotted_occupations[word] = project_between_two_points(model[word], model['softball'], model['baseball'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ballplayer', 0.8659656),\n",
       " ('major_leaguer', 0.8154428),\n",
       " ('philosopher', 0.46584326),\n",
       " ('broadcaster', 0.45277196),\n",
       " ('barber', 0.4252447),\n",
       " ('sportswriter', 0.39869338),\n",
       " ('inventor', 0.39034843),\n",
       " ('industrialist', 0.36241075),\n",
       " ('financier', 0.35488385),\n",
       " ('mathematician', 0.347327)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# close to baseball\n",
    "plotted_occupations.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('treasurer', -0.31703272),\n",
       " ('valedictorian', -0.3229477),\n",
       " ('bookkeeper', -0.34015563),\n",
       " ('goalkeeper', -0.35132754),\n",
       " ('homemaker', -0.36078987),\n",
       " ('secretary', -0.37140495),\n",
       " ('vocalist', -0.38780102),\n",
       " ('receptionist', -0.44150662),\n",
       " ('registered_nurse', -0.45217896),\n",
       " ('soloist', -0.46549433)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# close to softball\n",
    "plotted_occupations.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Geometry of Gender and Bias\n",
    "### 5.1 Identifying the gender subspace\n",
    "To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_normalized_direction(vec1, vec2):\n",
    "    center = (vec1 + vec2) / 2\n",
    "    return [vec1 - center, vec2 - center]\n",
    "\n",
    "gender_directions = []\n",
    "for f, m in definitional_pairs:\n",
    "    gender_directions += get_normalized_direction(model[f], model[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_vector(model):\n",
    "    vocab_size = len(model.vocab)\n",
    "    idx = random.randint(0, vocab_size-1)\n",
    "    return model.vectors[idx]\n",
    "\n",
    "random_directions = []\n",
    "for _ in range(len(definitional_pairs)):\n",
    "    random_directions += get_normalized_direction(get_random_vector(model), get_random_vector(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first eigenvalue is significantly larger than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(gender_directions)\n",
    "plt.bar(range(n_components), pca.explained_variance_ratio_)\n",
    "g = pca.components_[0]\n",
    "g /= np.linalg.norm(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(random_directions)\n",
    "plt.bar(range(n_components), pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Direct bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_direct_bias(neutral_embeddings: List[np.ndarray], g: np.ndarray, c: float=1.0):\n",
    "    neutral_embeddings = np.stack(neutral_embeddings)\n",
    "    cosine_sims = neutral_embeddings.dot(g)  # assume the vectors are normalized\n",
    "    return (abs(cosine_sims) ** c).sum() / len(neutral_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in w2vNEWS, if we take N to be the set of 327 occupations, then DirectBias1 = 0.08, which confirms that many occupation words have substantial component along the gender direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08050745259425576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profession_words = [word for word, _, _ in professions]\n",
    "profession_vectors = [model[w] for w in profession_words]\n",
    "compute_direct_bias(profession_vectors, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2922147829329026"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The direct bias of gender specific words\n",
    "gender_vectors = []\n",
    "for f, m in definitional_pairs:\n",
    "    gender_vectors.append(model[f])\n",
    "    gender_vectors.append(model[m])\n",
    "compute_direct_bias(gender_vectors, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05302191235665289"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The direct bias of random words\n",
    "random_vectors = [get_random_vector(model) for _ in range(1000)]\n",
    "compute_direct_bias(random_vectors, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Indirect bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indirect_bias(w, v, g):\n",
    "    dot_product = w.dot(v)\n",
    "    \n",
    "    def compute_orthogonal(vec, direction):\n",
    "        return vec - vec.dot(direction) * direction\n",
    "\n",
    "    orth_w = compute_orthogonal(w, g)\n",
    "    orth_v = compute_orthogonal(v, g)\n",
    "    \n",
    "    non_gender_sim = orth_w.dot(orth_v) / (np.linalg.norm(orth_w) * np.linalg.norm(orth_v))\n",
    "    \n",
    "    return (dot_product - non_gender_sim) / dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as _receptionist,_ _waitress_ and _homemaker_ are closer to\n",
    "_softball_ than _football,_ and the β’s between these words and _softball_ is substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723426377668333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_indirect_bias(model[\"receptionist\"], model['softball'], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3178421346299225"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_indirect_bias(model[\"waitress\"], model['softball'], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38374104380558116"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_indirect_bias(model[\"homemaker\"], model['softball'], g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Debiasing algorithms\n",
    "The first step, called Identify gender subspace, is to identify a direction (or, more generally, a subspace) of the embedding that captures the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Identify gender subspace\n",
    "def get_gender_subspace(model: KeyedVectors, defining_sets: List[Tuple[str, str]], k: int):\n",
    "    \n",
    "    def get_normalized_direction(vec1, vec2):\n",
    "        center = (vec1 + vec2) / 2\n",
    "        return [vec1 - center, vec2 - center]\n",
    "    \n",
    "    direction_vectors = []\n",
    "    for f, m in defining_sets:\n",
    "        direction_vectors += get_normalized_direction(model[f], model[m])\n",
    "\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(gender_directions)\n",
    "    return pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Step 2a: Hard de-biasing\n",
    "\n",
    "def project(vec, B):\n",
    "    subspace_magnitude = vec.dot(B.T)\n",
    "    vec_projected = subspace_magnitude.dot(B)\n",
    "    return vec_projected\n",
    "\n",
    "def neutralize(model, B, stop_list = None, in_place = False):\n",
    "    \n",
    "    if not in_place:\n",
    "        model = copy.deepcopy(model)\n",
    "    \n",
    "    if stop_list is not None:\n",
    "        stop_list_indices = [model.vocab[word].index for word in stop_list]\n",
    "        stop_word_embeddings = model.vectors[stop_list_indices] \n",
    "\n",
    "    all_vectors_projected = project(model.vectors, B)\n",
    "    model.vectors -= all_vectors_projected\n",
    "    model.vectors /= np.linalg.norm(model.vectors, axis=1)[:, np.newaxis, ]\n",
    "    if stop_list is not None:\n",
    "        model.vectors[stop_list_indices] = stop_word_embeddings\n",
    "    model.init_sims(replace=True)\n",
    "    return model\n",
    "\n",
    "def equalize(model, B, equality_sets, in_place: bool = False):\n",
    "    if not in_place:\n",
    "        model = copy.deepcopy(model)\n",
    "\n",
    "    for w1, w2 in equality_sets:\n",
    "        mean = (model[w1] + model[w2]) / 2\n",
    "        mean_projected = project(mean, B)\n",
    "        mean_neutral = mean - mean_projected\n",
    "        w1_projected = project(model[w1], B)\n",
    "        w2_projected = project(model[w2], B)\n",
    "        \n",
    "        w1_diff = w1_projected - mean_projected\n",
    "        w2_diff = w2_projected - mean_projected\n",
    "        \n",
    "        model[w1] = mean_neutral + np.sqrt(1 - (mean_neutral**2).sum()) * w1_diff / np.linalg.norm(w1_diff)\n",
    "        model[w2] = mean_neutral + np.sqrt(1 - (mean_neutral**2).sum()) * w2_diff / np.linalg.norm(w2_diff)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "equality_words = []\n",
    "for s in equalize_pairs:\n",
    "    equality_words += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_gender_subspace(model, definitional_pairs, 3)\n",
    "debiased = neutralize(model, B, equality_words)\n",
    "debiased = equalize(debiased, B, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homemaker', 0.5627118945121765),\n",
       " ('housewife', 0.5105046033859253),\n",
       " ('graphic_designer', 0.5051802396774292),\n",
       " ('schoolteacher', 0.497949481010437),\n",
       " ('businesswoman', 0.49348917603492737),\n",
       " ('paralegal', 0.49255111813545227),\n",
       " ('registered_nurse', 0.4907974600791931),\n",
       " ('saleswoman', 0.4881627559661865),\n",
       " ('electrical_engineer', 0.4797726571559906),\n",
       " ('mechanical_engineer', 0.4755399525165558)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'computer_programmer'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mechanical_engineer', 0.5537900328636169),\n",
       " ('electrical_engineer', 0.5340908765792847),\n",
       " ('businesswoman', 0.49205249547958374),\n",
       " ('engineer', 0.47469794750213623),\n",
       " ('carpenter', 0.45996394753456116),\n",
       " ('programmer', 0.45308178663253784),\n",
       " ('graphic_designer', 0.4483581781387329),\n",
       " ('homemaker', 0.4454195499420166),\n",
       " ('schoolteacher', 0.44264787435531616),\n",
       " ('machinist', 0.44043925404548645)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased.most_similar(positive=['woman', 'computer_programmer'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "0.2159463\n",
      "0.40326244\n",
      "After\n",
      "0.30415044714199124\n",
      "0.30415044521171064\n"
     ]
    }
   ],
   "source": [
    "word = \"homemaker\"\n",
    "print(\"Before\")\n",
    "print(model.similarity(word, \"man\"))\n",
    "print(model.similarity(word, \"woman\"))\n",
    "print(\"After\")\n",
    "print(debiased.similarity(word, \"man\"))\n",
    "print(debiased.similarity(word, \"woman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
